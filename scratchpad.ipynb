{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.misc import imread\n",
    "from alexnet import AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sign_names = pd.read_csv('signnames.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Load traffic signs data.\n",
    "with open('train.p', mode='rb') as f:\n",
    "  data = pickle.load(f)\n",
    "\n",
    "X_data, y_data = data['features'], data['labels']\n",
    "n_classes = len(np.unique(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Split data into training and validation sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Define placeholders and resize operation.\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "x_resized = tf.image.resize_images(x, (227, 227))\n",
    "\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "y_one_hot = tf.one_hot(y, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: pass placeholder as first argument to `AlexNet`.\n",
    "fc7 = AlexNet(x_resized, feature_extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# NOTE: `tf.stop_gradient` prevents the gradient from flowing backwards\n",
    "# past this point, keeping the weights before and up to `fc7` frozen.\n",
    "# This also makes training faster, less work to do!\n",
    "fc7 = tf.stop_gradient(fc7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Add the final layer for traffic sign classification.\n",
    "shape = (fc7.get_shape().as_list()[-1], n_classes)  # use this shape for the weight matrix\n",
    "\n",
    "fc8w = tf.Variable(tf.truncated_normal(shape), name='fc8w')\n",
    "fc8b = tf.Variable(tf.zeros(n_classes), name='fc8b')\n",
    "\n",
    "logits = tf.matmul(fc7, fc8w) + fc8b\n",
    "probs = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Define loss, training, accuracy operations.\n",
    "# HINT: Look back at your traffic signs project solution, you may\n",
    "# be able to reuse some the code.\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, y_one_hot)\n",
    "cost_function = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "rate = 0.003\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(cost_function)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_one_hot, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(X_data, y_data, sess):\n",
    "    n_samples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    # sess = tf.get_default_session()\n",
    "    for offset in range(0, n_samples, BATCH_SIZE):\n",
    "        end = offset + BATCH_SIZE\n",
    "        x_batch, y_batch = X_train[offset:end], y_train[offset:end]\n",
    "        batch_accuracy = sess.run(accuracy_operation, feed_dict={x: x_batch, y: y_batch})\n",
    "        total_accuracy += (batch_accuracy*len(x_batch))\n",
    "    return total_accuracy/n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Train and evaluate the feature extraction model.\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "save_file = 'Saved_model.ckpt'\n",
    "init = tf.global_variables_initializer()\n",
    "n_samples = len(X_train)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "if not os.path.isfile(save_file+'.index'):\n",
    "  sess.run(init)\n",
    "  print('Training...')\n",
    "  print()\n",
    "  for i in range(EPOCHS):\n",
    "    print(\"EPOCH {} ...\".format(i+1))\n",
    "    # train model\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    for offset in range(0, n_samples, BATCH_SIZE):\n",
    "        end = offset + BATCH_SIZE\n",
    "        print('\\tBatch {}-{} ...'.format(offset, end))\n",
    "        x_batch, y_batch = X_train[offset:end], y_train[offset:end]\n",
    "        sess.run(training_operation, feed_dict={x: x_batch, y: y_batch})\n",
    "\n",
    "    saver.save(sess,'Saved_model.ckpt')\n",
    "\n",
    "    training_accuracy = evaluate(X_train, y_train, sess)\n",
    "    testing_accuracy = evaluate(X_test, y_test, sess)\n",
    "    print()\n",
    "    print(\"Training Accuracy = {:.3f}, Validation Accuracy = {:.3f}\".format(training_accuracy, testing_accuracy))\n",
    "\n",
    "    # saver.save(sess, './saved_models/lenet'+str(i+1))\n",
    "    # print(\"Model-\"+str(i+1)+\" saved\")\n",
    "    print()\n",
    "else:\n",
    "  print('Restoring saved model...')\n",
    "  saver.restore(sess, './'+save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's evaluate based on the previous images...\n",
    "# Read Images\n",
    "im1 = imread(\"construction.jpg\").astype(np.float32)\n",
    "im1 = im1 - np.mean(im1)\n",
    "\n",
    "im2 = imread(\"stop.jpg\").astype(np.float32)\n",
    "im2 = im2 - np.mean(im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run Inference\n",
    "t = time.time()\n",
    "output = sess.run(probs, feed_dict={x: [im1, im2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Print Output\n",
    "for input_im_ind in range(output.shape[0]):\n",
    "    inds = np.argsort(output)[input_im_ind, :]\n",
    "    print(\"Image\", input_im_ind)\n",
    "    for i in range(5):\n",
    "        print(\"%s: %.3f\" % (sign_names.ix[inds[-1 - i]][1], output[input_im_ind, inds[-1 - i]]))\n",
    "    print()\n",
    "\n",
    "print(\"Time: %.3f seconds\" % (time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
